{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63a06a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92f4e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ System initialized. Connected to ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found. Please check your .env file.\")\n",
    "\n",
    "google_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n",
    "    api_key=api_key,\n",
    "    model_name=\"models/text-embedding-004\"\n",
    ")\n",
    "\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"mkdocs_db/\")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"MkDocsGuide\",\n",
    "    embedding_function=google_ef\n",
    ")\n",
    "\n",
    "print(\"‚úÖ System initialized. Connected to ChromaDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d5ac63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaning and splitting logic defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML carousels, image tags, and excessive whitespace\n",
    "    specific to the MkDocs user guide.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'<div id=\"mkdocs-theme-images\".*?</div>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    \n",
    "    text = re.sub(r'', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Cleaning and splitting logic defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f876005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Found 9 Markdown files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 254.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processing complete. Prepared 144 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_texts = []\n",
    "all_metadatas = []\n",
    "all_ids = []\n",
    "\n",
    "md_files = glob.glob(\"user-guide/*.md\")\n",
    "\n",
    "print(f\"üìÇ Found {len(md_files)} Markdown files.\")\n",
    "\n",
    "for file_path in tqdm(md_files, desc=\"Processing Files\"):\n",
    "  \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "\n",
    "    cleaned_content = clean_text(content)\n",
    "    \n",
    "    md_splits = md_splitter.split_text(cleaned_content)\n",
    "    \n",
    "    final_splits = rec_splitter.split_documents(md_splits)\n",
    "    \n",
    "    file_name = os.path.basename(file_path)\n",
    "    \n",
    "    for i, doc in enumerate(final_splits):\n",
    "\n",
    "        header_path = \" > \".join([v for k, v in doc.metadata.items() if k.startswith(\"Header\")])\n",
    "        enriched_text = f\"File: {file_name}\\nSection: {header_path}\\nContent:\\n{doc.page_content}\"\n",
    "        \n",
    "        all_texts.append(enriched_text)\n",
    "        \n",
    "        meta = doc.metadata.copy()\n",
    "        meta[\"source\"] = file_name\n",
    "        all_metadatas.append(meta)\n",
    "        \n",
    "        all_ids.append(f\"{file_name}-chunk-{i}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Processing complete. Prepared {len(all_texts)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adccc9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting insertion of 144 chunks...\n",
      "   Processing batch 0 to 20...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 20 to 40...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 40 to 60...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 60 to 80...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 80 to 100...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 100 to 120...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 120 to 140...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "   Processing batch 140 to 144...\n",
      "      ‚è© Batch already exists. Skipping.\n",
      "\n",
      "üéâ Database creation successful! You can now run RAG.py.\n"
     ]
    }
   ],
   "source": [
    "def batch_insert(texts, metadatas, ids, batch_size=20):\n",
    "    total = len(texts)\n",
    "    print(f\"üöÄ Starting insertion of {total} chunks...\")\n",
    "    \n",
    "    for i in range(0, total, batch_size):\n",
    "        batch_end = min(i + batch_size, total)\n",
    "        batch_texts = texts[i:batch_end]\n",
    "        batch_metas = metadatas[i:batch_end]\n",
    "        batch_ids = ids[i:batch_end]\n",
    "        \n",
    "        print(f\"   Processing batch {i} to {batch_end}...\")\n",
    "        \n",
    "        \n",
    "        existing = collection.get(ids=batch_ids)\n",
    "        if len(existing['ids']) == len(batch_ids):\n",
    "            print(\"      ‚è© Batch already exists. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "        \n",
    "       \n",
    "        collection.add(\n",
    "            documents=batch_texts,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "        print(\"      ‚úÖ Retry successful.\")\n",
    "            \n",
    "        \n",
    "        time.sleep(1.5)\n",
    "\n",
    "batch_insert(all_texts, all_metadatas, all_ids)\n",
    "\n",
    "print(\"\\nüéâ Database creation successful! You can now run RAG.py.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
